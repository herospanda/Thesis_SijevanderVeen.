{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can sentiment be predicted on French Twitter data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading needed python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk  \n",
    "from sklearn.datasets import load_files  \n",
    "nltk.download('stopwords')  \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.preprocessing import text\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comment data is spread over 2 collums, because the data contains also ´,´ which is used as delimter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/sije/Thesis/data/French-Sentiment-Analysis-Dataset-master/tweets.csv\", delimiter = \",\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df[['comment1', 'comment2']].apply(lambda x: ''.join(str(x)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows which have missing values are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consits out two labels and the numbers are almost equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          43306\n",
       "4          38355\n",
       "L'homme        1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stemming function is applies a French steming on the data. Also unwanted data is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "def get_stemming(sen):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(sen))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.stem(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_stemmer'] = df['comment'].apply(get_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is split in training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df['message_stemmer'], df['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label are created and the target variable are encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 data transformations are created word level tf-idf, ngram level tf-idf and characters level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['message_stemmer'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_tfidf =  tfidf_vect.transform(X_test.apply(lambda x: np.str_(x)))\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(df['message_stemmer'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test.apply(lambda x: np.str_(x)))\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(df['message_stemmer'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test.apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(df['message_stemmer'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_count =  count_vect.transform(X_test.apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model function which will be used for all different models, which returns a confusion matrix, classifaction report and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    print(confusion_matrix(y_test,predictions))  \n",
    "    print(classification_report(y_test,predictions))  \n",
    "    print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors: \n",
      "[[8139 2644]\n",
      " [4287 5346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70     10783\n",
      "           1       0.67      0.55      0.61      9633\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     20416\n",
      "   macro avg       0.66      0.65      0.65     20416\n",
      "weighted avg       0.66      0.66      0.66     20416\n",
      "\n",
      "0.6605113636363636\n",
      "NB, WordLevel TF-IDF: \n",
      "[[8030 2753]\n",
      " [4279 5354]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.74      0.70     10783\n",
      "           1       0.66      0.56      0.60      9633\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     20416\n",
      "   macro avg       0.66      0.65      0.65     20416\n",
      "weighted avg       0.66      0.66      0.65     20416\n",
      "\n",
      "0.655564263322884\n",
      "NB, N-Gram Vectors: \n",
      "[[7465 3318]\n",
      " [3889 5744]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67     10783\n",
      "           1       0.63      0.60      0.61      9633\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     20416\n",
      "   macro avg       0.65      0.64      0.64     20416\n",
      "weighted avg       0.65      0.65      0.65     20416\n",
      "\n",
      "0.6469925548589341\n",
      "NB, CharLevel Vectors: \n",
      "[[9839  944]\n",
      " [1102 8531]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91     10783\n",
      "           1       0.90      0.89      0.89      9633\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     20416\n",
      "   macro avg       0.90      0.90      0.90     20416\n",
      "weighted avg       0.90      0.90      0.90     20416\n",
      "\n",
      "0.8997844827586207\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "print(\"NB, Count Vectors: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_count, y_train, xvalid_count)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "print(\"NB, WordLevel TF-IDF: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "print(\"NB, N-Gram Vectors: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "print(\"NB, CharLevel Vectors: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, y_train, xvalid_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7421 3362]\n",
      " [3525 6108]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68     10783\n",
      "           1       0.64      0.63      0.64      9633\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     20416\n",
      "   macro avg       0.66      0.66      0.66     20416\n",
      "weighted avg       0.66      0.66      0.66     20416\n",
      "\n",
      "0.6626665360501567\n",
      "LR, WordLevel TF-IDF: \n",
      "[[7555 3228]\n",
      " [3597 6036]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69     10783\n",
      "           1       0.65      0.63      0.64      9633\n",
      "\n",
      "   micro avg       0.67      0.67      0.67     20416\n",
      "   macro avg       0.66      0.66      0.66     20416\n",
      "weighted avg       0.67      0.67      0.67     20416\n",
      "\n",
      "0.6657033699059561\n",
      "LR, N-Gram Vectors: \n",
      "[[7435 3348]\n",
      " [3847 5786]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67     10783\n",
      "           1       0.63      0.60      0.62      9633\n",
      "\n",
      "   micro avg       0.65      0.65      0.65     20416\n",
      "   macro avg       0.65      0.65      0.65     20416\n",
      "weighted avg       0.65      0.65      0.65     20416\n",
      "\n",
      "0.647580329153605\n",
      "LR, CharLevel Vectors: \n",
      "[[9745 1038]\n",
      " [  74 9559]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.95     10783\n",
      "           1       0.90      0.99      0.95      9633\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     20416\n",
      "   macro avg       0.95      0.95      0.95     20416\n",
      "weighted avg       0.95      0.95      0.95     20416\n",
      "\n",
      "0.9455329153605015\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "print(\"LR, Count Vectors: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count, y_train, xvalid_count)\n",
    "\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "print(\"LR, WordLevel TF-IDF: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
    "\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "print(\"LR, N-Gram Vectors: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
    "\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "print(\"LR, CharLevel Vectors: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, y_train, xvalid_tfidf_ngram_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10783     0]\n",
      " [ 9633     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69     10783\n",
      "           1       0.00      0.00      0.00      9633\n",
      "\n",
      "   micro avg       0.53      0.53      0.53     20416\n",
      "   macro avg       0.26      0.50      0.35     20416\n",
      "weighted avg       0.28      0.53      0.37     20416\n",
      "\n",
      "0.528164184952978\n",
      "SVM, N-Gram Vectors: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8175 2608]\n",
      " [5123 4510]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.76      0.68     10783\n",
      "           1       0.63      0.47      0.54      9633\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     20416\n",
      "   macro avg       0.62      0.61      0.61     20416\n",
      "weighted avg       0.62      0.62      0.61     20416\n",
      "\n",
      "0.6213264106583072\n",
      "RF, Count Vectors: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8077 2706]\n",
      " [5128 4505]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.75      0.67     10783\n",
      "           1       0.62      0.47      0.53      9633\n",
      "\n",
      "   micro avg       0.62      0.62      0.62     20416\n",
      "   macro avg       0.62      0.61      0.60     20416\n",
      "weighted avg       0.62      0.62      0.61     20416\n",
      "\n",
      "0.6162813479623824\n",
      "RF, WordLevel TF-IDF: None\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, y_train, xvalid_count)\n",
    "print(\"RF, Count Vectors: \" + str(accuracy))\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Linear classifcation using CharLevel Vectors results in an accuracy 0.95. Therefore it is the best scoring model. With an accurancy of 0.95 can be said that a sentiment model can be made for French twitter data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
