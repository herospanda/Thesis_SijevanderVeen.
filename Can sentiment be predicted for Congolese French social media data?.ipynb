{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can sentiment be predicted for Congolese French Facebook data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading needed python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sije/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import re  \n",
    "import nltk  \n",
    "from sklearn.datasets import load_files  \n",
    "nltk.download('stopwords')  \n",
    "import pickle  \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, numpy, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.preprocessing import text\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data, ony message which have more than 3 words are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3044: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/sije/Thesis/DRC_topics.csv\", delimiter = ';')\n",
    "\n",
    "df = df[df['event.message'].notnull()]\n",
    "def get_length(message):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    message_size = len(message)\n",
    "    return int(message_size)\n",
    "\n",
    "convert_string = {'anger': 4, 'disgust': 1, 'fear': 7, 'happiness': 6,'neutral': 3,'sadness': 5,'surprise': 2}\n",
    "\n",
    "def get_integer_emotion(emotion):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    new_integer = convert_string[emotion] \n",
    "    return int(new_integer)\n",
    "\n",
    "df['size_message'] = df['event.message'].apply(get_length)\n",
    "df['emotion_integer'] = df['event.emotion'].apply(get_integer_emotion)\n",
    "df = df[df['size_message'] > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stemming function is applies a French steming on the data. Also unwanted data is removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "stemmer = FrenchStemmer()\n",
    "def get_stemming(sen):  \n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(sen))\n",
    "\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.stem(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    return(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_stemmer'] = df['event.message'].apply(get_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number for each clas are printed. The zero class is almost three times bigger than the positive and negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0    209513\n",
      " 1     73513\n",
      "-1     60788\n",
      "Name: event.polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['event.polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(df['message_stemmer'], df['event.polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 data transformations are created word level tf-idf, ngram level tf-idf and characters level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df['message_stemmer'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_tfidf =  tfidf_vect.transform(X_test.apply(lambda x: np.str_(x)))\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(df['message_stemmer'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_test.apply(lambda x: np.str_(x)))\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(df['message_stemmer'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test.apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is vecotrized, this the data without any extra datatransformation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(df['message_stemmer'])\n",
    "xtrain_count =  count_vect.transform(X_train.apply(lambda x: np.str_(x)))\n",
    "xvalid_count =  count_vect.transform(X_test.apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    print(confusion_matrix(y_test,predictions))  \n",
    "    print(classification_report(y_test,predictions))  \n",
    "    print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors: \n",
      "[[ 3854  9239  2095]\n",
      " [ 4477 41017  7024]\n",
      " [ 1889  9799  6560]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.25      0.30     15188\n",
      "           1       0.68      0.78      0.73     52518\n",
      "           2       0.42      0.36      0.39     18248\n",
      "\n",
      "   micro avg       0.60      0.60      0.60     85954\n",
      "   macro avg       0.49      0.46      0.47     85954\n",
      "weighted avg       0.57      0.60      0.58     85954\n",
      "\n",
      "0.5983549340344836\n",
      "NB, WordLevel TF-IDF: \n",
      "[[ 1217 13404   567]\n",
      " [  459 50802  1257]\n",
      " [  345 15306  2597]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.08      0.14     15188\n",
      "           1       0.64      0.97      0.77     52518\n",
      "           2       0.59      0.14      0.23     18248\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     85954\n",
      "   macro avg       0.61      0.40      0.38     85954\n",
      "weighted avg       0.62      0.64      0.54     85954\n",
      "\n",
      "0.6354096377131955\n",
      "NB, N-Gram Vectors: \n",
      "[[ 2401 11800   987]\n",
      " [ 1452 49044  2022]\n",
      " [  802 13916  3530]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.16      0.24     15188\n",
      "           1       0.66      0.93      0.77     52518\n",
      "           2       0.54      0.19      0.28     18248\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     85954\n",
      "   macro avg       0.57      0.43      0.43     85954\n",
      "weighted avg       0.61      0.64      0.57     85954\n",
      "\n",
      "0.6395862903413454\n",
      "NB, CharLevel Vectors: \n",
      "[[ 1832 12589   767]\n",
      " [  701 49610  2207]\n",
      " [  630 14649  2969]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.12      0.20     15188\n",
      "           1       0.65      0.94      0.77     52518\n",
      "           2       0.50      0.16      0.25     18248\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     85954\n",
      "   macro avg       0.57      0.41      0.40     85954\n",
      "weighted avg       0.60      0.63      0.56     85954\n",
      "\n",
      "0.6330246410870931\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "print(\"NB, Count Vectors: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_count, y_train, xvalid_count)\n",
    "\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "print(\"NB, WordLevel TF-IDF: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "print(\"NB, N-Gram Vectors: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
    "\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "print(\"NB, CharLevel Vectors: \")\n",
    "train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, y_train, xvalid_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4915  8914  1181]\n",
      " [ 2495 46491  3578]\n",
      " [ 1534 11732  5114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.33      0.41     15010\n",
      "           1       0.69      0.88      0.78     52564\n",
      "           2       0.52      0.28      0.36     18380\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     85954\n",
      "   macro avg       0.59      0.50      0.52     85954\n",
      "weighted avg       0.63      0.66      0.62     85954\n",
      "\n",
      "0.6575610210112386\n",
      "LR, WordLevel TF-IDF: \n",
      "[[ 5082  8432  1496]\n",
      " [ 2015 47904  2645]\n",
      " [ 1308 11219  5853]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.34      0.43     15010\n",
      "           1       0.71      0.91      0.80     52564\n",
      "           2       0.59      0.32      0.41     18380\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     85954\n",
      "   macro avg       0.63      0.52      0.55     85954\n",
      "weighted avg       0.66      0.68      0.65     85954\n",
      "\n",
      "0.6845405682109035\n",
      "LR, N-Gram Vectors: \n",
      "[[ 3174 10648  1188]\n",
      " [ 1420 49309  1835]\n",
      " [  998 13473  3909]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.21      0.31     15010\n",
      "           1       0.67      0.94      0.78     52564\n",
      "           2       0.56      0.21      0.31     18380\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     85954\n",
      "   macro avg       0.60      0.45      0.47     85954\n",
      "weighted avg       0.63      0.66      0.60     85954\n",
      "\n",
      "0.6560718523861601\n",
      "LR, CharLevel Vectors: \n",
      "[[ 6073  7468  1469]\n",
      " [ 2150 47915  2499]\n",
      " [ 1595 10602  6183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.40      0.49     15010\n",
      "           1       0.73      0.91      0.81     52564\n",
      "           2       0.61      0.34      0.43     18380\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     85954\n",
      "   macro avg       0.65      0.55      0.58     85954\n",
      "weighted avg       0.68      0.70      0.67     85954\n",
      "\n",
      "0.7000372292156269\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "print(\"LR, Count Vectors: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_count, y_train, xvalid_count)\n",
    "\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "print(\"LR, WordLevel TF-IDF: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
    "\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "print(\"LR, N-Gram Vectors: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
    "\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "print(\"LR, CharLevel Vectors: \")\n",
    "train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, y_train, xvalid_tfidf_ngram_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7024  6357  1738]\n",
      " [ 4292 45367  2727]\n",
      " [ 2729 10048  5672]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.46      0.48     15119\n",
      "           1       0.73      0.87      0.79     52386\n",
      "           2       0.56      0.31      0.40     18449\n",
      "\n",
      "   micro avg       0.68      0.68      0.68     85954\n",
      "   macro avg       0.60      0.55      0.56     85954\n",
      "weighted avg       0.66      0.68      0.65     85954\n",
      "\n",
      "0.6755124834213649\n",
      "RF, Count Vectors: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sije/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7192  6269  1658]\n",
      " [ 2737 47254  2395]\n",
      " [ 2331 10191  5927]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.48      0.53     15119\n",
      "           1       0.74      0.90      0.81     52386\n",
      "           2       0.59      0.32      0.42     18449\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     85954\n",
      "   macro avg       0.64      0.57      0.59     85954\n",
      "weighted avg       0.68      0.70      0.68     85954\n",
      "\n",
      "0.702387323452079\n",
      "RF, WordLevel TF-IDF: None\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, y_train, xvalid_count)\n",
    "print(\"RF, Count Vectors: \" + str(accuracy))\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, y_train, xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest using TF-IDF results in an accuracy 0.702. Therefore it is the best scoring model. There is a difference between macro and micro average of 11 procent. This could be optimized by using a smaller set of the neutral class. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
